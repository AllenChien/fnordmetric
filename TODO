
-> msync
-> import freelist properly
-> MmapPageManager, MallocPageManager, InMemoryDatabase, FileBackedDatabase
-> crc32 instead of fnc32 for checksums?
-> file lock on open
-> proper error handling

You can run it as a standalone timerseries database or as a network of distributed
data collection agents. SQL Syntax, statsd support, c++, bindings for x,z,y jadda
jadda

FnordMetric is heavily optimized for sequential queries and aggregations over
time ranges. This means running something like "give me the slowest pages by
90th percentile latency for every 10 minute window in the last 3 days" is pretty
fast, whereas something like "give me a whole stream with thousands of rows but
reorder it by some custom" field is better suited for a regular database.

- streaming queries that output a continiously changing dataset
- standing queries (< streaming queries) that output the data into a new stream
  as it is generated
- streaming queries can't have: order by -- can they?
- no sub-query support! (if you need sub queries what you are trying to do is
probably better suited for a regular database anyway)

Execution Flow:

  {
     Scan+Filter,
     Aggregate
  }
  Having
  Re-Order
  Limit, Offset


-- Select number of http error codes in the last hour
SELECT http_status, count(http_status) from http_requests
  WHERE time > -24hours
  GROUP BY http_status;
-- AGGREGATE_LOCKSTEP;

-- Select 90th percentile latency in the last day
SELECT percentile(latency, 90) from latency_metric WHERE time > -24hours;
-- SCAN_LOCKSTEP

-- SELECT error rate with two metrics and a 5 min window
SELECT
  time,
  ((delta(succesful_requests.count) / delta(errors.count)) * 100) as error_rate,
  FROM successful_requests, errors
  WHERE time > -24hours
  GROUP BY TIME_WINDOW(5 minutes);
-- AGGREGATE_TIME_WINDOW

-- SELECT top 10 slowest pages yesterday by 90th percentile latency
SELECT
  url, percentile(90, latency) as 90thpercentile_latency
  FROM request_log
  WHERE time > 24hours
  GROUP BY url
  ORDER BY 90thpercentile_latency DESC
-- SELECT all pages yesterday with a 90th percentile latency > 1000ms
SELECT
  url, percentile(90, latency) as 90thpercentile_latency
  FROM request_log
  WHERE time > 24hours
  GROUP BY url
  HAVING 90thpercentile_latency > 1000;

-- SELECT 90thpercentile page latency for three pages in 5 min windows
SELECT
  time, url, percentile(90, latency) as 90thpercentile_latency
  FROM request_log
  WHERE time > 24hours
  WHERE url IN ("/mypage1", "/mypage2", "/mypage3")
  GROUP BY TIME_WINDOW(5minutes), url;

-- COUNT number of events per time period
SELECT count(*) FROM metric GROUP BY TIME_WINDOW(5minutes);

-- SELECT holtwinters forecast for a metric value
SELECT real_value, holtwinters_forecast(real_value)
  FROM metric
  WHERE time > -24hours AND time < +24hours;

-- JOIN two streams on a non time parameter
-- SELECT http error rate from one metric for yesterday in 5 min windows
-- SELECT http error rate from one metric for yesterday gross


-> Concepts: Stream, Agents vs Server, Retention Policy

-> functions: sum, avg, percentile, mean, variance, stddev, delta, nth_derivate,
              holtwinters

-> msync safety mode: relaxed (consistent but might loose data), conservative
                      (msync async) and paranoid (msync sync)

-> implicit fields: time, agent, offset

-> UI menu: query / streams
-> streams: local/remote. disk usage, replication status, retention, etc
-> query ui: tabs for: html req equivalent, html embed, etc
-> query safety mode: on/off (check row checksums)
-> malloc page manager
-> example:
  * getting started standalone:
     * create http_reqs stream with statuscode, count
     * select select http error rate
  * getting started distributed
     * agents have metrics with http req time distribution and url
     * graph global req time distribution
     * graph 90th percentile req time per agent
-> renice query threads in agents?
-> annotations / deploy lines?
-> push query down into backend
-> drop-in statsd adapter :)
-> agents that run queries on external data sources (e.g. logfiles)
-> newrelic-like agents that auto-export streams on demand
-> on demand streams :)
-> nagios adapter for alerting
-> http interface? (need to bundle lotsa resources)
-> query cache! :)
-> stream replication
-> optimized for seeks over time ranges
-> row compression / page compression?
-> interactive query UI generates embed html snippets
-> generate embed snippets from ruby api -> rails helper plugin
-> execute multiple queries in parallel in the ui -> get multiple timeseries
   and tables (tabs)
