
Ways to contribute:
  + report bugs or suggest new features
  + suggest or implement new options for existing charts or new chart types
  + add a new default stylesheet (requires css knowledge)
  + add some new features to the query engine (requires c++ knowledge)
  + add a new query backend (requires c++ knowledge)
  + add a new language binding (requires minimal c++ knowledge)
  + add a new render target (requires c++ knowledge)
  + many more! improve all the things :)

If you need any help or guidance please contact the mailing list.

This is how you run a development build and the tests:
   ....


DRAW {BAR|LINE|AREA} CHART
  [TITLE title [subtitle]]
  [XAXIS {BOTTOM|TOP|BOTH|AUTO|OFF} [label]]
  [YAXIS {LEFT|RIGHT|BOTH|AUTO|OFF} [label]]
  [STACKED {ON|OFF}]
  [ORIENTATION {HORIZONTAL|VERTICAL}]
  [LABELS {INSIDE|OUTSIDE|AUTO|OFF} [ROTATE] [color]]

SERIES
    series_name_expr
    [AXIS expr AS {X|Y|Z} [SCALE min_expr, max_expr]]
    [COLOR color_expr]
    [LABEL label_expr]
    FROM select_statement;

SELECT
    [ALL | DISTINCT | DISTINCTROW ]
    select_expr [, select_expr ...]
    [FROM table_references
    [WHERE where_condition]
    [GROUP BY {col_name | expr | position}
      [ASC | DESC], ...]
    [HAVING where_condition]
    [ORDER BY {col_name | expr | position}
      [ASC | DESC], ...]
    [LIMIT {[offset,] row_count | row_count OFFSET offset}];


As the first simple example we will create a bar chart that displays gross
domestic product per country and looks like this:

     #
     #       #
     #       #      #
     #       #      #
     USA     UK     IRAN ...
  -----------------------------------------

Let's say we have a csv file containing the data that looks like this:

  country,gdp
  iran,8000
  uk,9000
  usa,1000

We can create the chart pictured above with this sql query"

  -- import the csv file as a table
  IMPORT gdp_per_country FROM "gdp_per_country.csv";

  -- draw a simple bar chart
  DRAW BAR CHART;

  -- draw gdp of the top 10 countries
  SERIES "gdp per country" FROM
    SELECT gbp, country FROM gdp_per_country ORDER BY gdp DESC LIMIT 10;


Now lets add a custom style and axis deinitions:

  -- draw a simple bar chart
  DRAW BAR CHART
    WITH STYLE ...;

  -- draw gdp of the top 10 countries
  SERIES "gdp per country" FROM
    SELECT gbp, country FROM gdp_per_country ORDER BY gdp DESC LIMIT 10;
    WITH gdp AS X and country as Y;


Another example:

Draw a few latency distributions for multiple loadtest experiments sourced from
multiple csv files. The csv files contain two columns "time" and "runtime" and
one line for each request in the load test. The "time" column contains the wall
clock time at which the request was issued and the "runtime" column contains the
runtime of the request.

We will draw a single line graph that plots requests per minute vs the 90th
percentile latency in that time window.

  IMPORT experiment1 FROM "experiment_1.csv";

  DRAW LINE CHART;

  SERIES "experiment1" FROM
    SELECT count(*) as qps, nth_percentile(runtime, 90) as latency
      FROM experiment1
      GROUP BY time_window(time, 1s);

Lets add another experiment as a second series in the chart:

  IMPORT experiment1 FROM "experiment_1.csv";
  IMPORT experiment2 FROM "experiment_2.csv";

  DRAW LINE CHART;

  SERIES "experiment1" FROM
    SELECT count(*) as qps, nth_percentile(runtime, 90) as latency
      FROM experiment1
      GROUP BY time_window(time, 1s);

  SERIES "experiment2" FROM
    SELECT count(*) as qps, nth_percentile(runtime, 90) as latency
      FROM experiment2
      GROUP BY time_window(time, 1s);


To make that last query more interesting lets say we put all our test results
into a single file with an additional "experiment" columns and try to recreate
the same chart:

-- load a csv file with per request logs from a load test
IMPORT loadtests FROM "myfile.csv";

-- draw a graph of qps vs 90thpercentile, one series for each experiment
DRAW LINE CHART;

SERIES experiment FROM
  SELECT experiment, count(*) as qps, nth_percentile(runtime) as latency
    FROM loadtests
    GROUP BY experiment, time_window(time, 1s);




-- draw a graph with explicit axis definition
DRAW LINE CHART;

SERIES experiment XAXIS fu YAXIS bar FROM
  SELECT fu, bar FROM table1;


-- draw a graph with different y scales per series
DRAW LINE CHART;

SERIES experiment YSCALE 0, 100 FROM
  SELECT x, y FROM table1;

SERIES experiment YSCALE 0, 500 FROM
  SELECT x, y FROM table2;


-- draw a graph with dynamic YSCALE
DRAW LINE CHART;

SERIES experiment YSCALE 0, max(y) * 1.5 FROM
  SELECT x, y FROM table1;

SERIES experiment YSCALE 0, max(y) * 1.5 FROM
  SELECT x, y FROM table2;


Examples page with examples left: query, right: output and link to examples/
folder with source data!

eMail contributers and long time followers "FnordMetric <3 username"


-> msync
-> import freelist properly
-> MmapPageManager, MallocPageManager, InMemoryDatabase, FileBackedDatabase
-> crc32 instead of fnc32 for checksums?
-> file lock on open
-> proper error handling

You can run it as a standalone timerseries database or as a network of distributed
data collection agents. SQL Syntax, statsd support, c++, bindings for x,z,y jadda
jadda

FnordMetric is heavily optimized for sequential queries and aggregations over
time ranges. This means running something like "give me the slowest pages by
90th percentile latency for every 10 minute window in the last 3 days" is pretty
fast, whereas something like "give me a whole stream with thousands of rows but
reorder it by some custom" field is better suited for a regular database.

- streaming queries that output a continiously changing dataset
- standing queries (< streaming queries) that output the data into a new stream
  as it is generated
- streaming queries can't have: order by -- can they?
- no sub-query support! (if you need sub queries what you are trying to do is
probably better suited for a regular database anyway)

Execution Flow:

  {
     Scan+Filter,
     Aggregate
  }
  Having
  Re-Order
  Limit, Offset


-- Select number of http error codes in the last hour
SELECT http_status, count(http_status) from http_requests
  WHERE time > -24hours
  GROUP BY http_status;
-- AGGREGATE_LOCKSTEP;

-- Select 90th percentile latency in the last day
SELECT percentile(latency, 90) from latency_metric WHERE time > -24hours;
-- SCAN_LOCKSTEP

-- SELECT error rate with two metrics and a 5 min window
SELECT
  time,
  ((delta(succesful_requests.count) / delta(errors.count)) * 100) as error_rate,
  FROM successful_requests, errors
  WHERE time > -24hours
  GROUP BY TIME_WINDOW(5 minutes);
-- AGGREGATE_TIME_WINDOW

-- SELECT top 10 slowest pages yesterday by 90th percentile latency
SELECT
  url, percentile(90, latency) as 90thpercentile_latency
  FROM request_log
  WHERE time > 24hours
  GROUP BY url
  ORDER BY 90thpercentile_latency DESC
-- SELECT all pages yesterday with a 90th percentile latency > 1000ms
SELECT
  url, percentile(90, latency) as 90thpercentile_latency
  FROM request_log
  WHERE time > 24hours
  GROUP BY url
  HAVING 90thpercentile_latency > 1000;

-- SELECT 90thpercentile page latency for three pages in 5 min windows
SELECT
  time, url, percentile(90, latency) as 90thpercentile_latency
  FROM request_log
  WHERE time > 24hours
  WHERE url IN ("/mypage1", "/mypage2", "/mypage3")
  GROUP BY TIME_WINDOW(5minutes), url;

-- COUNT number of events per time period
SELECT count(*) FROM metric GROUP BY TIME_WINDOW(5minutes);

-- SELECT holtwinters forecast for a metric value
SELECT real_value, holtwinters_forecast(real_value)
  FROM metric
  WHERE time > -24hours AND time < +24hours;

-- JOIN two streams on a non time parameter
-- SELECT http error rate from one metric for yesterday in 5 min windows
-- SELECT http error rate from one metric for yesterday gross


-> Concepts: Stream, Agents vs Server, Retention Policy

-> functions: sum, avg, percentile, mean, variance, stddev, delta, nth_derivate,
              holtwinters

-> msync safety mode: relaxed (consistent but might loose data), conservative
                      (msync async) and paranoid (msync sync)

-> implicit fields: time, agent, offset

-> UI menu: query / streams
-> streams: local/remote. disk usage, replication status, retention, etc
-> query ui: tabs for: html req equivalent, html embed, etc
-> query safety mode: on/off (check row checksums)
-> malloc page manager
-> example:
  * getting started standalone:
     * create http_reqs stream with statuscode, count
     * select select http error rate
  * getting started distributed
     * agents have metrics with http req time distribution and url
     * graph global req time distribution
     * graph 90th percentile req time per agent
-> renice query threads in agents?
-> annotations / deploy lines?
-> push query down into backend
-> drop-in statsd adapter :)
-> agents that run queries on external data sources (e.g. logfiles)
-> newrelic-like agents that auto-export streams on demand
-> on demand streams :)
-> nagios adapter for alerting
-> http interface? (need to bundle lotsa resources)
-> query cache! :)
-> stream replication
-> optimized for seeks over time ranges
-> row compression / page compression?
-> interactive query UI generates embed html snippets
-> generate embed snippets from ruby api -> rails helper plugin
-> execute multiple queries in parallel in the ui -> get multiple timeseries
   and tables (tabs)
